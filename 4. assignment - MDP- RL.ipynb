{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('dsfs': conda)"
  },
  "interpreter": {
   "hash": "d9e452968c4b4c5d4f08d8743a9e3451cc489f424ad8efada9572444e3c86427"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "import mdptoolbox.mdp as mdp\n",
    "import mdptoolbox.util as util\n",
    "import mdptoolbox.example as example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model MDP flexibly, depending on size of warehouse, number of items, probabilities for items \n",
    "#model needs:\n",
    "    #set of states\n",
    "    #set of actions\n",
    "    #transition probabilities\n",
    "    #reward matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input changes, layout of warehouse, nr of items and probability distribution of items\n",
    "warehouse = np.zeros(shape=(1,2)) #shape larger than 2x3 will take 30 plus mins to calculate transition matrix and policies\n",
    "items = [(1, 0.9), (2, 0.1)]\n",
    "\n",
    "\n",
    "items_only = []\n",
    "for item in items:\n",
    "    items_only.append(item[0])\n",
    "probabilities_only = []\n",
    "for item in items:\n",
    "    probabilities_only.append(item[1])\n",
    "#check sum of probabilities\n",
    "if np.sum(probabilities_only) != 1:\n",
    "    raise ValueError(\"The sum of item probabilities is not 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "108 states\n3 actions\n"
     ]
    }
   ],
   "source": [
    "# states:\n",
    "boxstates = items_only.copy()\n",
    "boxstates.insert(0,0)\n",
    "tasks = [\"store\", \"unstore\"]\n",
    "nr_boxes = warehouse.shape[0]*warehouse.shape[1]\n",
    "nr_states = len(boxstates)**nr_boxes * len(tasks) * len(items)\n",
    "iterables = []\n",
    "\n",
    "#same set for every box\n",
    "for i in range(nr_boxes):\n",
    "    iterables.append(boxstates)\n",
    "\n",
    "#add iterables for tasks\n",
    "iterables.append(tasks)\n",
    "iterables.append(items_only)\n",
    "\n",
    "states = []\n",
    "counter = 0\n",
    "for state in itertools.product(*iterables):\n",
    "    counter += 1\n",
    "    states.append(list(state))\n",
    "\n",
    "#actions:\n",
    "#action is selecting the position in the warehouse for the given task\n",
    "rows = list(np.arange(warehouse.shape[0]))\n",
    "columns = list(np.arange(warehouse.shape[1]))\n",
    "nr_cells = warehouse.shape[0]*warehouse.shape[1]\n",
    "actions = []\n",
    "for action in itertools.product(*[rows, columns]):\n",
    "    actions.append(list(action))\n",
    "\n",
    "nr_actions = len(actions)\n",
    "\n",
    "print(nr_states,\"states\")\n",
    "print(nr_actions, \"actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "action: 0\naction: 1\naction: 2\nWall time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#transitions and rewards:\n",
    "#this could be refactored and made pretty for sure!\n",
    "#matrix AxSxS', every state has probability to get to state s' by each action\n",
    "#reward depends on whether task was possible before or s==s' and distance that is determined by action\n",
    "\n",
    "transitions = []\n",
    "rewards = np.zeros(shape=(len(states), len(actions)))\n",
    "it_prob = 0\n",
    "\n",
    "for it_action in range(nr_actions):\n",
    "    print(\"action:\", it_action)\n",
    "    #position in warehouse\n",
    "    y,x = actions[it_action]\n",
    "    # for CSR matrix\n",
    "    data = []\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    for it_state in range(nr_states):\n",
    "        #check for every state, if action is doable    \n",
    "        state = states[it_state].copy()\n",
    "        if state[-2] == \"store\":\n",
    "            if state[y*warehouse.shape[0]+x] == 0:\n",
    "                #item can be stored at position\n",
    "                result = state.copy()\n",
    "                result[y*warehouse.shape[0]+x] = state[-1]                \n",
    "                #reward = 1/distance, so everyhing is positive valued and linear\n",
    "                #distance = row + column + 1 (minimal distance = 1)\n",
    "                distance = y+x+1\n",
    "                rewards[it_state, it_action] = 1/distance\n",
    "                #identify correct state in list and set probability according to item\n",
    "                for next_state in range(nr_states):\n",
    "                    #iterate twice (store and unstore) over item probabilities and set transition probability accordingly\n",
    "                    prob = probabilities_only[it_prob]/2\n",
    "                    if (result[0:-2] == states[next_state][0:-2]):           \n",
    "                        it_prob += 1\n",
    "                        if it_prob == len(probabilities_only):\n",
    "                            it_prob = 0\n",
    "                        data.append(prob)\n",
    "                        indices.append(next_state)\n",
    "                indptr.append(len(indices))\n",
    "\n",
    "            else:\n",
    "                #reward[it_action, it_state]\n",
    "                result = state.copy()\n",
    "                #identify correct state in list and set probability according to item\n",
    "                for next_state in range(nr_states):\n",
    "                    if (result == states[next_state]):           \n",
    "                        data.append(1)\n",
    "                        indices.append(next_state)    \n",
    "                indptr.append(len(indices))\n",
    "            \n",
    "        else:\n",
    "            if state[y*warehouse.shape[0]+x] == state[-1]:\n",
    "                #can be unstored\n",
    "                result = state.copy()\n",
    "                result[y*warehouse.shape[0]+x] = 0\n",
    "                distance = y+x+1\n",
    "                rewards[it_state, it_action] = 1/distance\n",
    "                #identify correct state in list and set probability according to item\n",
    "                for next_state in range(nr_states):\n",
    "                    #iterate twice (store and unstore) over item probabilities and set transition probability accordingly\n",
    "                    prob = probabilities_only[it_prob]/2\n",
    "                    if (result[0:-2] == states[next_state][0:-2]):           \n",
    "                        it_prob += 1\n",
    "                        if it_prob == len(probabilities_only):\n",
    "                            it_prob = 0\n",
    "                        data.append(prob)\n",
    "                        indices.append(next_state)\n",
    "                indptr.append(len(indices))\n",
    "\n",
    "            else:\n",
    "                #reward[it_action, it_state]\n",
    "                result = state.copy()\n",
    "                #identify correct state in list and set probability according to item\n",
    "                for next_state in range(nr_states):\n",
    "                    if (result == states[next_state]):           \n",
    "                        data.append(1)\n",
    "                        indices.append(next_state)\n",
    "                indptr.append(len(indices))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    transition_sparse = csr_matrix((data, indices, indptr), shape=(nr_states ,nr_states), dtype=float)\n",
    "    transitions.append(transition_sparse)\n",
    "\n",
    "\n",
    "# check if transitions and rewards form a valid MDP (sizewise at least)\n",
    "if util.check(transitions, rewards) is not None or len(transitions) != nr_actions or not all( i for i in [transitions[i].shape[0]==transitions[i].shape[1]==nr_states for i in range(len(transitions))]):\n",
    "    raise ValueError(\"Size of Transition or reward matrix not correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = transitions[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different MDP algorithms\n",
    "    #FiniteHorizon\n",
    "    #PolicyIteraiton\n",
    "    #PolicyIterationModified\n",
    "    #QLearning\n",
    "    #RelativeValueIteration\n",
    "    #ValueIteration\n",
    "    #ValueIteartionGS\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\"FiniteHorizon\", \"PolicyIteraiton\", \"PolicyIterationModified\", \"QLearning\", \"RelativeValueIteration\", \"ValueIteration\", \"ValueIteartionGS\"]\n",
    "policies = [None,None,None,None,None,None,None]\n",
    "discountFactor = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fh duration iterations: 51.11390829086304\n"
     ]
    }
   ],
   "source": [
    "#FiniteHorizon\n",
    "fh = mdp.FiniteHorizon(transitions, rewards, discountFactor, N=10000)\n",
    "fh.run()\n",
    "print(\"fh duration iterations:\", fh.time)\n",
    "#use iteration of policy\n",
    "policy = []\n",
    "policy_iterations = fh.policy\n",
    "for state in policy_iterations:\n",
    "    policy.append(state[0])\n",
    "policies[0] = (tuple(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pi duration: 0.0040130615234375\n"
     ]
    }
   ],
   "source": [
    "#PolicyIteraiton\n",
    "pi = mdp.PolicyIteration(transitions, rewards, discountFactor, max_iter=10000000)\n",
    "pi.run()\n",
    "policies[1] = pi.policy\n",
    "print(\"pi duration:\", pi.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pim duration: 0.0040132999420166016\n"
     ]
    }
   ],
   "source": [
    "#PolicyIterationModified\n",
    "pim = mdp.PolicyIterationModified(transitions, rewards, discountFactor, max_iter=1000000)\n",
    "pim.run()\n",
    "policies[2] = pim.policy\n",
    "print(\"pim duration:\", pim.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ql duration: 78.76735591888428\n"
     ]
    }
   ],
   "source": [
    "#QLearning\n",
    "ql = mdp.QLearning(transitions, rewards, discountFactor, n_iter = 50000)\n",
    "ql.run()\n",
    "policies[3] = ql.policy\n",
    "print(\"ql duration:\", ql.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rvi duration: 0.0010030269622802734\n"
     ]
    }
   ],
   "source": [
    "#RelativeValueIteration\n",
    "rvi = mdp.RelativeValueIteration(transitions, rewards, max_iter=200000)\n",
    "rvi.run()\n",
    "policies[4] = rvi.policy\n",
    "print(\"rvi duration:\", rvi.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vi duration: 0.0010023117065429688\n"
     ]
    }
   ],
   "source": [
    "#ValueIteration\n",
    "vi = mdp.ValueIteration(transitions, rewards, discountFactor, max_iter=2000000)\n",
    "vi.run()\n",
    "policies[5] = vi.policy\n",
    "print(\"vi duration:\", vi.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vigs duration: 0.5116922855377197\n"
     ]
    }
   ],
   "source": [
    "#ValueIteartionGS\n",
    "vigs = mdp.ValueIterationGS(transitions, rewards, discountFactor, max_iter=100000)\n",
    "vigs.run()\n",
    "policies[6] = vigs.policy\n",
    "print(\"vigs duration:\", vigs.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(policy, state):\n",
    "    if len(policy)!=len(states):\n",
    "        return \"Policy does not match number of states!\"\n",
    "    #give state and get action(position based on chosen policy)\n",
    "    for it_state in range(len(states)):\n",
    "        #find position of state to search policy at this posiiton \n",
    "        if states[it_state]==state:\n",
    "            return actions[policy[it_state]]\n",
    "    return \"Input state is invalid, check shape and items!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warehouse input length: 3\nPossible tasks: ['store', 'unstore']\nPossible items: [1, 2]\n"
     ]
    }
   ],
   "source": [
    "#to display options to test out states\n",
    "print(\"Warehouse input length:\", warehouse.shape[0]*warehouse.shape[1])\n",
    "print(\"Possible tasks:\", tasks)\n",
    "print(\"Possible items:\", items_only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Possible actions: [[0, 0], [0, 1], [0, 2]]\nFor state [0, 0, 0, 'store', 2] policy FiniteHorizon [0, 1]\nFor state [0, 0, 0, 'store', 2] policy PolicyIteraiton [0, 1]\nFor state [0, 0, 0, 'store', 2] policy PolicyIterationModified [0, 1]\nFor state [0, 0, 0, 'store', 2] policy QLearning [0, 0]\nFor state [0, 0, 0, 'store', 2] policy RelativeValueIteration [0, 1]\nFor state [0, 0, 0, 'store', 2] policy ValueIteration [0, 1]\nFor state [0, 0, 0, 'store', 2] policy ValueIteartionGS [0, 1]\n"
     ]
    }
   ],
   "source": [
    "#experiemnt herer with this state and policy\n",
    "test_state = [0, 0, 0, 'store', 2]\n",
    "\n",
    "print(\"Possible actions:\", actions)\n",
    "for i in range(len(policies)):\n",
    "    print(\"For state\", test_state,\"policy\", algorithms[i], get_action(policies[i], test_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the end"
   ]
  }
 ]
}