{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('dsfs': conda)"
  },
  "interpreter": {
   "hash": "d9e452968c4b4c5d4f08d8743a9e3451cc489f424ad8efada9572444e3c86427"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import itertools\n",
    "import numpy as np\n",
    "import mdptoolbox.mdp as mdp\n",
    "import mdptoolbox.util as util\n",
    "import mdptoolbox.example as example\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model MDP flexibly, depending on size of warehouse, number of items, probabilities for items \n",
    "#model needs:\n",
    "    #set of states\n",
    "    #set of actions\n",
    "    #transition probabilities\n",
    "    #reward matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this input to experiment\n",
    "warehouse = np.zeros(shape=(2, 2),dtype=int) #shape larger than 2x3 will take 30 plus mins to calculate transition matrix and policies\n",
    "items = [(1, 0.9), (2, 0.1)]\n",
    "\n",
    "\n",
    "\n",
    "# dont change this \n",
    "tasks = [\"store\", \"unstore\"]\n",
    "greedy_items = []\n",
    "for item in range(len(items)):\n",
    "    greedy_items.append((items[item][0], 1/len(items)))\n",
    "\n",
    "greedy_probabilities_only = []\n",
    "for item in greedy_items:\n",
    "    greedy_probabilities_only.append(item[1])    \n",
    "\n",
    "items_only = []\n",
    "for item in items:\n",
    "    items_only.append(item[0])\n",
    "\n",
    "probabilities_only = []\n",
    "for item in items:\n",
    "    probabilities_only.append(item[1])\n",
    "#check sum of probabilities\n",
    "if np.sum(probabilities_only) != 1:\n",
    "    raise ValueError(\"The sum of item probabilities is not 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "states and actions done\n"
     ]
    }
   ],
   "source": [
    "def get_states_actions(warehouse, items):\n",
    "\n",
    "    items_only = []\n",
    "    for item in items:\n",
    "        items_only.append(item[0])\n",
    "\n",
    "    probabilities_only = []\n",
    "    for item in items:\n",
    "        probabilities_only.append(item[1])\n",
    "\n",
    "    #states    \n",
    "    boxstates = items_only.copy()\n",
    "    boxstates.insert(0,0)\n",
    "    \n",
    "    nr_boxes = warehouse.shape[0]*warehouse.shape[1]\n",
    "    nr_states = len(boxstates)**nr_boxes * len(tasks) * len(items)\n",
    "    iterables = []\n",
    "\n",
    "    #same set for every box\n",
    "    for i in range(nr_boxes):\n",
    "        iterables.append(boxstates)\n",
    "\n",
    "    #add iterables for tasks\n",
    "    iterables.append(tasks)\n",
    "    iterables.append(items_only)\n",
    "\n",
    "    states = []\n",
    "    counter = 0\n",
    "    for state in itertools.product(*iterables):\n",
    "        counter += 1\n",
    "        states.append(list(state))\n",
    "\n",
    "    #actions\n",
    "    #action is selecting the position in the warehouse for the given task\n",
    "    rows = list(np.arange(warehouse.shape[0]))\n",
    "    columns = list(np.arange(warehouse.shape[1]))\n",
    "    nr_cells = warehouse.shape[0]*warehouse.shape[1]\n",
    "    actions = []\n",
    "    for action in itertools.product(*[rows, columns]):\n",
    "        actions.append(list(action))\n",
    "    print(\"states and actions done\")\n",
    "    return states, actions\n",
    "\n",
    "\n",
    "#instanciate    \n",
    "states, actions = get_states_actions(warehouse, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "action: 0\naction: 1\naction: 2\naction: 3\ntransitions and rewards done\naction: 0\naction: 1\naction: 2\naction: 3\ntransitions and rewards done\nWall time: 171 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_tranitions_rewards(states, actions, probabilities_only=probabilities_only):\n",
    "    #transitions and rewards:\n",
    "    #this could be refactored and made pretty for sure!\n",
    "    #matrix AxSxS', every state has probability to get to state s' by each action\n",
    "    #reward depends on whether task was possible before or s==s' and distance that is determined by action\n",
    "    transitions = []\n",
    "    rewards = np.zeros(shape=(len(states), len(actions)))\n",
    "    it_prob = 0\n",
    "\n",
    "    for it_action in range(len(actions)):\n",
    "        print(\"action:\", it_action)\n",
    "        #position in warehouse\n",
    "        y,x = actions[it_action]\n",
    "        # for CSR matrix\n",
    "        data = []\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        for it_state in range(len(states)):\n",
    "            #check for every state, if action is doable    \n",
    "            state = states[it_state].copy()\n",
    "            if state[-2] == \"store\":\n",
    "                if state[y*warehouse.shape[0]+x] == 0:\n",
    "                    #item can be stored at position\n",
    "                    result = state.copy()\n",
    "                    result[y*warehouse.shape[0]+x] = state[-1]                \n",
    "                    #reward = 1/distance, so everyhing is positive valued and linear\n",
    "                    #distance = row + column + 1 (minimal distance = 1)\n",
    "                    distance = y+x+1\n",
    "                    rewards[it_state, it_action] = 1/distance\n",
    "                    #identify correct state in list and set probability according to item\n",
    "                    for next_state in range(len(states)):\n",
    "                        #iterate twice (store and unstore) over item probabilities and set transition probability accordingly\n",
    "                        prob = probabilities_only[it_prob]/2\n",
    "                        if (result[0:-2] == states[next_state][0:-2]):           \n",
    "                            it_prob += 1\n",
    "                            if it_prob == len(probabilities_only):\n",
    "                                it_prob = 0\n",
    "                            data.append(prob)\n",
    "                            indices.append(next_state)\n",
    "                    indptr.append(len(indices))\n",
    "\n",
    "                else:\n",
    "                    #reward[it_action, it_state]\n",
    "                    result = state.copy()\n",
    "                    #identify correct state in list and set probability according to item\n",
    "                    for next_state in range(len(states)):\n",
    "                        if (result == states[next_state]):           \n",
    "                            data.append(1)\n",
    "                            indices.append(next_state)    \n",
    "                    indptr.append(len(indices))\n",
    "                \n",
    "            else:\n",
    "                if state[y*warehouse.shape[0]+x] == state[-1]:\n",
    "                    #can be unstored\n",
    "                    result = state.copy()\n",
    "                    result[y*warehouse.shape[0]+x] = 0\n",
    "                    distance = y+x+1\n",
    "                    rewards[it_state, it_action] = 1/distance\n",
    "                    #identify correct state in list and set probability according to item\n",
    "                    for next_state in range(len(states)):\n",
    "                        #iterate twice (store and unstore) over item probabilities and set transition probability accordingly\n",
    "                        prob = probabilities_only[it_prob]/2\n",
    "                        if (result[0:-2] == states[next_state][0:-2]):           \n",
    "                            it_prob += 1\n",
    "                            if it_prob == len(probabilities_only):\n",
    "                                it_prob = 0\n",
    "                            data.append(prob)\n",
    "                            indices.append(next_state)\n",
    "                    indptr.append(len(indices))\n",
    "\n",
    "                else:\n",
    "                    #reward[it_action, it_state]\n",
    "                    result = state.copy()\n",
    "                    #identify correct state in list and set probability according to item\n",
    "                    for next_state in range(len(states)):\n",
    "                        if (result == states[next_state]):           \n",
    "                            data.append(1)\n",
    "                            indices.append(next_state)\n",
    "                    indptr.append(len(indices))\n",
    "\n",
    "        transition_sparse = csr_matrix((data, indices, indptr), shape=(len(states) ,len(states)), dtype=float)\n",
    "        transitions.append(transition_sparse)\n",
    "\n",
    "    # check if transitions and rewards form a valid MDP (sizewise at least)\n",
    "    if util.check(transitions, rewards) is not None or len(transitions) != len(actions) or not all( i for i in [transitions[i].shape[0]==transitions[i].shape[1]==len(states) for i in range(len(transitions))]):\n",
    "        raise ValueError(\"Size of Transition or reward matrix not correct!\")\n",
    "    print(\"transitions and rewards done\")\n",
    "    return transitions, rewards\n",
    "\n",
    "\n",
    "#instanciate    \n",
    "transitions, rewards = get_tranitions_rewards(states, actions)\n",
    "g_trans, g_rewards = get_tranitions_rewards(states, actions, probabilities_only=greedy_probabilities_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different MDP algorithms and get the optimal policy\n",
    "    #FiniteHorizon\n",
    "    #PolicyIteraiton\n",
    "    #PolicyIterationModified\n",
    "    #QLearning\n",
    "    #RelativeValueIteration\n",
    "    #ValueIteration\n",
    "    #ValueIteartionGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "greedy duration 4.903049945831299\n",
      "pi duration: 5.309864044189453\n",
      "pim duration: 0.005013942718505859\n",
      "rvi duration: 0.002005338668823242\n",
      "vi duration: 0.004012584686279297\n",
      "vigs duration: 3.3375635147094727\n",
      "policies done\n"
     ]
    }
   ],
   "source": [
    "def get_policy(transitions, rewards, algorithms=[\"PolicyIteration\", \"PolicyIterationModified\", \"RelativeValueIteration\", \"ValueIteration\", \"ValueIteartionGS\"]):\n",
    "    #FiniteHorizon and QLearning seem not to converge to the optimal policy somehow....different results than other ones\n",
    "    policies = {}\n",
    "    discountFactor = 0.99\n",
    "\n",
    "    #add greedy policy, algorithm doesnt matter becasue probabilitites are uniform\n",
    "    greedy = mdp.PolicyIteration(g_trans, g_rewards, discountFactor, max_iter=1000)\n",
    "    greedy.run()\n",
    "    policies[\"GreedyPolicy\"] = greedy.policy\n",
    "    print(\"greedy duration\", greedy.time)\n",
    "    \n",
    "\n",
    "    if \"FiniteHorizon\" in algorithms:\n",
    "        #FiniteHorizon\n",
    "        iterations = 10000\n",
    "        fh = mdp.FiniteHorizon(transitions, rewards, discountFactor, N=iterations)\n",
    "        fh.run()\n",
    "        print(\"fh duration:\", fh.time)\n",
    "        #use iteration of policy\n",
    "        policy = []\n",
    "        policy_iterations = fh.policy\n",
    "        for state in policy_iterations:\n",
    "            policy.append(state[iterations-1])\n",
    "        policies[\"FiniteHorizon\"] = tuple(policy)\n",
    "    \n",
    "    if \"PolicyIteration\" in algorithms:\n",
    "        #PolicyIteraiton\n",
    "        pi = mdp.PolicyIteration(transitions, rewards, discountFactor, max_iter=1000)\n",
    "        pi.run()\n",
    "        policies[\"PolicyIteration\"] = pi.policy\n",
    "        print(\"pi duration:\", pi.time)\n",
    "\n",
    "    if \"PolicyIterationModified\" in algorithms:\n",
    "        #PolicyIterationModified\n",
    "        pim = mdp.PolicyIterationModified(transitions, rewards, discountFactor, max_iter=100000)\n",
    "        pim.run()\n",
    "        policies[\"PolicyIterationModified\"] = pim.policy\n",
    "        print(\"pim duration:\", pim.time)\n",
    "\n",
    "    if \"QLearning\" in algorithms:\n",
    "        #QLearning not used\n",
    "        ql = mdp.QLearning(transitions, rewards, discountFactor, n_iter = 10000)\n",
    "        ql.run()\n",
    "        policies[\"QLearning\"] = ql.policy\n",
    "        print(\"ql duration:\", ql.time)\n",
    "\n",
    "    if \"RelativeValueIteration\" in algorithms:\n",
    "        #RelativeValueIteration\n",
    "        rvi = mdp.RelativeValueIteration(transitions, rewards, max_iter=20000)\n",
    "        rvi.run()\n",
    "        policies[\"RelativeValueIteration\"] = rvi.policy\n",
    "        print(\"rvi duration:\", rvi.time)\n",
    "\n",
    "    if \"ValueIteration\" in algorithms:\n",
    "        #ValueIteration\n",
    "        vi = mdp.ValueIteration(transitions, rewards, discountFactor, max_iter=2000000)\n",
    "        vi.run()\n",
    "        policies[\"ValueIteration\"] = vi.policy\n",
    "        print(\"vi duration:\", vi.time)\n",
    "\n",
    "    if \"ValueIteartionGS\" in algorithms:\n",
    "        #ValueIteartionGS\n",
    "        vigs = mdp.ValueIterationGS(transitions, rewards, discountFactor, max_iter=1000000)\n",
    "        vigs.run()\n",
    "        policies[\"ValueIteartionGS\"] = vigs.policy\n",
    "        print(\"vigs duration:\", vigs.time)\n",
    "    \n",
    "    print(\"policies done\")\n",
    "    return policies\n",
    "\n",
    "#instanciate\n",
    "policies = get_policy(transitions, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rec_action(policy, state):\n",
    "    if len(policy)!=len(states):\n",
    "        return \"Policy does not match number of states!\"\n",
    "    #give state and get action(position based on chosen policy)\n",
    "    for it_state in range(len(states)):\n",
    "        #find position of state to search policy at this posiiton \n",
    "        if states[it_state]==state:\n",
    "            return actions[policy[it_state]]\n",
    "    return \"Input state is invalid, check shape and items!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warehouse input length: 4\nPossible actions: [[0, 0], [0, 1], [1, 0], [1, 1]]\nPossible tasks: store, unstore\nPossible items: [(1, 0.9), (2, 0.1)]\n"
     ]
    }
   ],
   "source": [
    "#to display options to test out states\n",
    "print(\"Warehouse input length:\", warehouse.shape[0]*warehouse.shape[1])\n",
    "print(\"Possible actions:\", actions)\n",
    "print(\"Possible tasks: store, unstore\")\n",
    "print(\"Possible items:\", items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For state [0, 0, 0, 0, 'store', 2] policy GreedyPolicy [0, 0]\nFor state [0, 0, 0, 0, 'store', 2] policy PolicyIteration [1, 0]\nFor state [0, 0, 0, 0, 'store', 2] policy PolicyIterationModified [0, 1]\nFor state [0, 0, 0, 0, 'store', 2] policy RelativeValueIteration [0, 1]\nFor state [0, 0, 0, 0, 'store', 2] policy ValueIteration [0, 1]\nFor state [0, 0, 0, 0, 'store', 2] policy ValueIteartionGS [0, 1]\n"
     ]
    }
   ],
   "source": [
    "#experiemnt here with this state and policy\n",
    "test_state = [0, 0, 0, 0,  'store', 2]\n",
    "\n",
    "\n",
    "for key in policies:\n",
    "    print(\"For state\", test_state,\"policy\", key,  get_rec_action(policies[key], test_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare greedy policy to the optimal policy by simulating a number of of connected warehouse states\n",
    "    # we can get the greedy policy by searching the optimal policy for items with a uniform probability distribution\n",
    "    # as this always prefers the closest free position/item for all items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(policy, iterations=100, randomstart=False, printer=False):\n",
    "    reward=[]\n",
    "\n",
    "    # either start with random state or empty warehouse with random storing task\n",
    "    if randomstart:\n",
    "        state = (random.choices(states, k=1)[0]).copy()\n",
    "    else:\n",
    "        warhouse_state = warehouse.tolist()[0]\n",
    "        warhouse_state.append(\"store\")\n",
    "        warhouse_state.append(random.choices(items_only, tuple(probabilities_only), k=1)[0])\n",
    "        state = warhouse_state.copy()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        if printer:\n",
    "            print()\n",
    "            print(\"iteration\", i+1)\n",
    "        # sanity checking states\n",
    "        # intervene for states that result in loops (impossible tasks)\n",
    "        # unstore item that is not in warehouse\n",
    "        if state[-2] == \"unstore\" and not (np.array(state[0:-2])==state[-1]).any():\n",
    "            state[-2] = \"store\"\n",
    "            if printer:\n",
    "                print(\"changed to store\", state)\n",
    "        # store item in full warehouse\n",
    "        if state [-2] == \"store\" and (np.array(state[0:-2])!=0).all():\n",
    "            state[-2]=\"unstore\"\n",
    "            if printer:\n",
    "                print(\"changed to unstore\", state)\n",
    "            # can neither unstore nor store then change item\n",
    "            if (np.array(state[0:-2])!=state[-1]).all():\n",
    "                state[-1] = random.choices(items_only, tuple(probabilities_only), k=1)[0]\n",
    "                if printer:\n",
    "                    print(\"changed item\", state)\n",
    "        \n",
    "        \n",
    "        #get recommended action based on policy and current state  \n",
    "        action = get_rec_action(policy, state)\n",
    "        if printer:\n",
    "            print(\"start\", state)\n",
    "            print(\"action\", action)         \n",
    "        \n",
    "        #find index of action and state\n",
    "        for it_action in range(len(actions)):\n",
    "            if action == actions[it_action]:\n",
    "                break       \n",
    "        for it_state in range(len(states)):\n",
    "            if state == states[it_state]:\n",
    "                break\n",
    "        \n",
    "        # add reward based on state and action\n",
    "        reward.append(rewards[it_state][it_action])\n",
    "        if printer:\n",
    "            print(\"reward\", rewards[it_state][it_action])\n",
    "        \n",
    "        # find possible next states(indices) from transition matrix\n",
    "        possible_next = list(np.where((transitions[it_action][it_state]).toarray() != 0)[1])\n",
    "        probabilites_next = []\n",
    "        for i in possible_next:\n",
    "            probabilites_next.append(transitions[it_action].toarray()[it_state][i])\n",
    "        if printer:\n",
    "            print(\"possible next\", possible_next)\n",
    "            print(\"probabilities next\", probabilites_next)\n",
    "\n",
    "        # pick next state based on assigned probability \n",
    "        it_state = random.choices(possible_next, probabilites_next, k=1)[0]\n",
    "        state = states[it_state].copy()\n",
    "        if printer:\n",
    "            print(\"result\",it_state, state)\n",
    "        \n",
    "        \n",
    "    #return average reward\n",
    "    return np.sum(reward)/iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\niteration 1\nchanged to unstore [1, 2, 1, 1, 'unstore', 2]\nstart [1, 2, 1, 1, 'unstore', 2]\naction [0, 1]\nreward 0.5\npossible next [124, 125, 126, 127]\nprobabilities next [0.45, 0.05, 0.45, 0.05]\nresult 127 [1, 0, 1, 1, 'unstore', 2]\n\niteration 2\nchanged to store [1, 0, 1, 1, 'store', 2]\nstart [1, 0, 1, 1, 'store', 2]\naction [0, 1]\nreward 0.5\npossible next [196, 197, 198, 199]\nprobabilities next [0.45, 0.05, 0.45, 0.05]\nresult 196 [1, 2, 1, 1, 'store', 1]\n\niteration 3\nchanged to unstore [1, 2, 1, 1, 'unstore', 1]\nstart [1, 2, 1, 1, 'unstore', 1]\naction [0, 0]\nreward 1.0\npossible next [88, 89, 90, 91]\nprobabilities next [0.45, 0.05, 0.45, 0.05]\nresult 90 [0, 2, 1, 1, 'unstore', 1]\n\niteration 4\nstart [0, 2, 1, 1, 'unstore', 1]\naction [1, 0]\nreward 0.5\npossible next [76, 77, 78, 79]\nprobabilities next [0.45, 0.05, 0.45, 0.05]\nresult 76 [0, 2, 0, 1, 'store', 1]\n\niteration 5\nstart [0, 2, 0, 1, 'store', 1]\naction [0, 0]\nreward 1.0\npossible next [184, 185, 186, 187]\nprobabilities next [0.45, 0.05, 0.45, 0.05]\nresult 186 [1, 2, 0, 1, 'unstore', 1]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "simulate(policies[\"GreedyPolicy\"], iterations=5, randomstart=True, printer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimized policy mean reward: 0.7463975\nGreedy policy mean reward: 0.6978208333333333\nWall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# compare greedy and optimized policy for a numer of simulations (if iterations is too short, difference will be hard to see if porbability of some items is too low)\n",
    "# larger number of simulations achieves greater stability\n",
    "greedy=[]\n",
    "optimized=[]\n",
    "for i in range(50):\n",
    "    optimized.append(simulate(policies[\"PolicyIteration\"], iterations=1000))\n",
    "    greedy.append(simulate(policies[\"GreedyPolicy\"], iterations=1000))\n",
    "\n",
    "print(\"Optimized policy mean reward:\", np.mean(optimized))\n",
    "print(\"Greedy policy mean reward:\", np.mean(greedy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the end"
   ]
  }
 ]
}